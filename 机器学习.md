机器学习:

特征处理:
数据归一化的原因:
	各个特征的量纲不一,会导致较小的量纲的那个特征的影响忽略不计,导致收敛变慢.
	归一化后可加快梯度下降的速度,提高精度
	方法:
		简单缩放,比如图片缩放到[0-1]

a.数据归一化常用方法:
	a1. z = (x - u)/r
		u:均值
		r:标准差
	a2. z = (x - min)/(max - min)
		min: 样本中最小值
		max: 样本中最大值
		取值范围0-1
	a3.神经网络中归一化方法:
		一半用wx+b后作为下层的输入
		x_matmul_w = tf.layers.batch_normalization(tf.matmul(X, w1) + b1, axis=1)
	a4.sklearn 归一化方法:
		from sklearn import prepeocessing
		# 标准化:a1
		X_scaled = preprocessing.scale(x)
		# a2
		minmaxscaler = preprocessing.MinMaxScaler()
		# 训练
		x_train_minmax = minmaxscaler.fit_transform(x_train)
		# 测试
		x_test_minmax = minmaxscaler.transform(x_test)

b.KNN:
	b1.算法步骤
		1.算出给定测试对象,计算与每一个训练样本对象的距离
		2.找出距离对象最近的k个对象
		3.这些邻近对象中的主要类别,对测试对象分类
	b2.可能遇到的问题
		1.如果训练数据集太大,导致效率底下,因为要遍历所有训练对象
		2.k值的选取,太小的话,如果测试对象周边找不到邻近对象,则会无法分类,
		3.维度太大,导致维度灾难,常用方法,降维,提取关键维度,
	b3.距离的加权
		等权与距离的倒数加权
		前者,每个邻近的权重都一样,选出最多的邻近的对象的类别作为分类
		后者,距离越远,权重越小
	b4.距离的算法
		1.欧几里得距离,差的平方和开根号
		2.余弦夹角,判断相似度,
		3.地理距离,差的平方和
	b5.k值的选取
		1.k值太小,容易受噪声干扰,k值太大,近邻中又有可能保肝太多其他类别的点,可选用距离加权
			k值通常采用交叉检验来确定,一半小于训练样本的平方根
	b6.高纬度对距离衡量的影响,变量阅读,欧氏距离区分就越差
	b7.变量值域对距离的影响,值域越大,计算距离时占的地位就越大,需要对样本标准化
	b8.模型简单,性能低下,一次性加载要遍历所有的样本,内存开销大
	b9.样本无需一视同仁,可对关键样本进行加权

c.决策树
	c1.算法步骤
		1.算出信息熵:
			H(x) = -sigm(P(Xi) log2 P(Xi))
		2.算出每个维度的信息条件熵
			H(D|A) = -sigm(Di/D P(Di)log2 P(Di))
		3.算出每个维度的信息增益(c4.5 采用信息增益率,克服了采用信息增益选择属性石偏向选择取值多的属性的不足)
			i(X) = H(X)-H(D|A)
		4.找出最大的信息增益作为枝
		5.划分数据集,
		6,重复1,2,345直至,不可划分
	c2.优缺点:
		缺:
		1,容易过拟合
		2,决策树不稳定,数据中一个数值的变化可能会生成一个完全不一样的数,可使用随机森林
		优:
		1.直观,容易理解
		2.训练的数据量不大
		3.复杂度不高
		4,无参数模型,
	c3.C4.5算法:
		1.连续数据离散化:
			1.1将连续的属性A的N个连续属性值,从小到大排序
			1.2二分法将数值分为两份,共有N-1中方法,取每一种分法的阈值对应的信息增益
			1.3取信息增益最大的最为划分
		2.剪枝.
			预剪枝:实用性不强,因为很难精确的判断何时终止

			后剪枝
				PEP算法:
					1.对于一个叶子节点,有n个样本,其中有e个错误,那么该叶子节点错误率为(e+0.5)/n,
						0.5为惩罚因子,
					2.对于一棵树,有L个叶子节点,那么该树的错误率为:
						error = sigm(e+ 0.5L)/sigm(n)
d.聚类
	K-means
	d1.算法步骤:
		1.定义K表示可以分成类别数量
		2.随机定义k质心
		3.样本中每个数据找到最近的那个质心
		4.将样本划分为k个
		5.分别计算k个簇的质心,将质心替换为新的质心
		6,重新计算每个样本到新的质心的距离
		7,重复3,4,5,6直至质心不再变化
	d2.优缺点:
		优点:
		1.快速,简单
		缺点:
		1.只有簇可以定义平均值的情况下才能使用
		2.K值很难确定,不知道有多少泪
		3.初始质心划分很重要,对聚类效果影响很大
		4,开销大,需要迭代所有样本
		5,对噪声敏感
		6,不适于划分非凸形状的簇,或者差别很小的簇
		7,不能保证最优全局解,只能保证局部最优解
	dbscan
		1.定义密度圆的半径,eps
		2.定义密度圆内的数量,minpts
		3,for 点p,算出p点半径eps内的集合N
			标记点p为visted
			if 数量Npts<minpts:
				标记改点为noise
			else:
				新建簇C,将集合N内的点全部归为簇C
				for p' in N:
					if Npts_ > minpts:
						对于p_ 新的集合N_
						N += N_
					if P_ not in C:
						add P_ to C

