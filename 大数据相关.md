1.hadoop:
2.map-reduce:
3.hive:
4.hbase:
5.storm:
6.
hbase开启thrift:
hbase-daemon.sh start/stop thrift2

hive启动模式:
1.本地mysql,安装配置见安装指南,启动hive
2.远程, 先启动metastroe: nohup hive --service metastore &
		再启动hiveserver: nohup hive --service hiveserver2 &
3.连接到hivesever2: !connect jdbc:hive2:host:10000/

4.启动mapreduce服务,端口10020:
	mr-jobhistory-daemon.sh start historyserver

5.mapreduce:
	本地验证:echo "foo foo quux labs foo bar quux" | python ./mapper.py
			echo "foo foo quux labs foo bar quux" | ./mapper.py | sort -k1,1 | ./reducer.py
	从windows上传的文件由于系统编码不同,需要将文件转码,
	用vim编辑器打开,:set ff=unix
	保存退出

	MR执行过程:执行的是标准输入输出
	1.mapper将文件分成很多分,教给每个节点
	2.每个mapper读取文件,执行mapper:读取每一行,按指定分割符提取每个词,输出<key,value>的形式,比如<hello, 1>
	3.reducer将每个mapper发过来的<key, value>进行排序分组,相当于group by 和 order,可能样子<a,1><a,1><b,1><c,1><c,1>
	4.执行我们编写的reducer:将每一组相同,我们做一个sum,输出
	tips:在python中itertools模块中有groupby方法,按照一定方法,满足方法的元素组合在一起,因为hadoop已经帮我排序拍好,切已经归类,
			就可以很方便的在在group中进行统计,
		groupby方法,返回两个,一个相同的元素a,以及连续包含这个元素的列表,一般写作,item,group<每个相同元素的集合>
		再对group迭代,sum即可

A.hadoop基本指令,web端口50070
 A1.上传文件:
 	A1.1:
 		新建目录input:
 		hdfs dfs -mkdir /input
 		上传文件
 		hdfs dfs -put /home/kaiseu/TianChi /input/
 		复制本地文件到hdfs
 		hdfs dfs -copyFromLOcal /txt /hdfsdir
 		移动文件到hdfs
 		hdfs dfs -moveFromLocal


B.hive相关概念:
		1.索引:
		hive的索引目的是提高Hive表指定列的查询速度,和sql数据库一样
		2.分区:
		分区是表的部分列的集合，可以为频繁使用的数据建立分区，这样查找分区中的数据时就不需要扫描全表，这对于提高查找效率很有帮助。

		分区是一种根据“分区列”（partition_column）的值对表进行粗略划分的机制。Hive中每个分区对应着表很多的子目录，将所有的数据按照分区列放入到不同的子目录中去

		分区的依据不能是表中真是存在的列,而是我们人为指定的伪列,比如存储日志,可以按照日期这个不存在的列进行分区

		3.分桶:
		分桶是指对表中已经存在的列进行hash运算,将相同的结果放在一个桶里

C.hive常用Hsql:
		1.建表,内表:
			CREATE [external] TABLE c02_clickstat_fatdt1 //external 外链表
			 (yyyymmdd string, id INT, ip string, country string, cookie_id string, page_id string , clickstat_url_id int, query_string string, refer string )
			//分表,伪列dt
			PARTITIONED BY(dt STRING) 
			// 行格式化类型
			row format delimited 
			//以换行符划分
			fields terminated by '\n'
			// 存储类型 
			stored as textfile;
			//指定分区
			loacation 'hdfs location'

			data_type
			  : primitive_type
			  | array_type
			  | map_type
			  | struct_type

			primitive_type
			  : TINYINT
			  | SMALLINT
			  | INT
			  | BIGINT
			  | BOOLEAN
			  | FLOAT
			  | DOUBLE
			  | STRING

			array_type
			  : ARRAY < data_type >

			map_type
			  : MAP < primitive_type, data_type >

			struct_type
			  : STRUCT < col_name : data_type [COMMENT col_comment], ...>

			row_format
			  : DELIMITED, COLLECTION, MAP KEYS, LINES TERMINATED BY char]
			  | SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, ...)]

			file_format:
			  : SEQUENCEFILE
			  | TEXTFILE
			  | RCFILE     (Note:  only available starting with 0.6.0)
			  | INPUTFORMAT input_format_classname OUTPUTFORMAT output_format_classname
		2.装在数据:
			LOAD DATA INPATH '/user/admin/SqlldrDat/CnClickstat/20131101/19/clickstat_gp_fatdt0/0'
			 OVERWRITE INTO TABLE c02_clickstat_fatdt1 PARTITION(dt='20131101');

		3.访问分区:
			SELECT count(*) FROM c02_clickstat_fatdt1 a WHERE a.dt >= '20131101' AND a.dt < '20131102';

		4.添加分区:
			ALTER TABLE c02_clickstat_fatdt1 ADD 
			PARTITION (dt='20131202') location '/user/hive/warehouse/c02_clickstat_fatdt1/part20131202' 
			PARTITION (dt='20131203') location '/user/hive/warehouse/c02_clickstat_fatdt1/part20131203';
		5.删除分区:
			ALTER TABLE c02_clickstat_fatdt1 DROP PARTITION (dt='20101202');
		6.重命名:
			ALTER TABLE table_name RENAME TO new_table_name
		7.修改列属性:
			修改类型,一般只增加长度:
			ALTER TABLE 表名 MODIFY(列名 数据类型);
			改名:
			ALTER TABLE 表名 RENAME COLUMN 当前名 TO 新名
			删除列:
			ALTER TABLE 表名 DROP COLUMN 列名

		8.视图:
			create view if not exits group_by_year_vw
			as
			select year,count(*) as video_ct from videos group by year;

		9.插入数据.只能从别的表插入到目标表:
			INSERT OVERWRITE TABLE events SELECT a.bar, count(*) FROM invites a WHERE a.foo > 0 GROUP BY a.bar;

		10.导出数据:
			INSERT OVERWRITE [LOCAL] DIRECTORY directory1 SELECT ... FROM ...
			
